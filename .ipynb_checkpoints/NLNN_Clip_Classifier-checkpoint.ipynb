{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import modules\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_processing_utils import *\n",
    "from NLNN import *\n",
    " \n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The dataset consists of fMRI time-series activations \n",
    "- 300 ROIs: schafer 300 ROI parcellation,\n",
    "- 176 participants,\n",
    "- each participant watches 15 movie clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clipno_overall</th>\n",
       "      <th>run</th>\n",
       "      <th>clipno_in_run</th>\n",
       "      <th>clip_name</th>\n",
       "      <th>start_tr</th>\n",
       "      <th>stop_tr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MOVIE1_7T_AP</td>\n",
       "      <td>1</td>\n",
       "      <td>twomen</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>264.0417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MOVIE1_7T_AP</td>\n",
       "      <td>2</td>\n",
       "      <td>bridgeville</td>\n",
       "      <td>284.0833</td>\n",
       "      <td>505.7083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MOVIE1_7T_AP</td>\n",
       "      <td>3</td>\n",
       "      <td>pockets</td>\n",
       "      <td>525.7500</td>\n",
       "      <td>713.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MOVIE1_7T_AP</td>\n",
       "      <td>4</td>\n",
       "      <td>overcome</td>\n",
       "      <td>733.7917</td>\n",
       "      <td>797.5417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MOVIE1_7T_AP</td>\n",
       "      <td>5</td>\n",
       "      <td>testretest1</td>\n",
       "      <td>817.5833</td>\n",
       "      <td>900.9583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>MOVIE2_7T_PA</td>\n",
       "      <td>1</td>\n",
       "      <td>inception</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>246.7083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>MOVIE2_7T_PA</td>\n",
       "      <td>2</td>\n",
       "      <td>socialnet</td>\n",
       "      <td>266.7500</td>\n",
       "      <td>525.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>MOVIE2_7T_PA</td>\n",
       "      <td>3</td>\n",
       "      <td>oceans</td>\n",
       "      <td>545.3750</td>\n",
       "      <td>794.5833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>MOVIE2_7T_PA</td>\n",
       "      <td>4</td>\n",
       "      <td>testretest2</td>\n",
       "      <td>814.5833</td>\n",
       "      <td>897.9583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>MOVIE3_7T_PA</td>\n",
       "      <td>1</td>\n",
       "      <td>flower</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>200.5417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>MOVIE3_7T_PA</td>\n",
       "      <td>2</td>\n",
       "      <td>hotel</td>\n",
       "      <td>220.5833</td>\n",
       "      <td>405.0833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>MOVIE3_7T_PA</td>\n",
       "      <td>3</td>\n",
       "      <td>garden</td>\n",
       "      <td>425.1250</td>\n",
       "      <td>629.2083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>MOVIE3_7T_PA</td>\n",
       "      <td>4</td>\n",
       "      <td>dreary</td>\n",
       "      <td>649.2500</td>\n",
       "      <td>791.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>MOVIE3_7T_PA</td>\n",
       "      <td>5</td>\n",
       "      <td>testretest3</td>\n",
       "      <td>811.5833</td>\n",
       "      <td>894.9583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>MOVIE4_7T_AP</td>\n",
       "      <td>1</td>\n",
       "      <td>homealone</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>252.2917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>MOVIE4_7T_AP</td>\n",
       "      <td>2</td>\n",
       "      <td>brokovich</td>\n",
       "      <td>272.3333</td>\n",
       "      <td>502.1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>MOVIE4_7T_AP</td>\n",
       "      <td>3</td>\n",
       "      <td>starwars</td>\n",
       "      <td>522.2083</td>\n",
       "      <td>777.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>MOVIE4_7T_AP</td>\n",
       "      <td>4</td>\n",
       "      <td>testretest4</td>\n",
       "      <td>797.5833</td>\n",
       "      <td>880.9583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    clipno_overall           run  clipno_in_run    clip_name  start_tr  \\\n",
       "0                1  MOVIE1_7T_AP              1       twomen   20.0000   \n",
       "1                2  MOVIE1_7T_AP              2  bridgeville  284.0833   \n",
       "2                3  MOVIE1_7T_AP              3      pockets  525.7500   \n",
       "3                4  MOVIE1_7T_AP              4     overcome  733.7917   \n",
       "4                5  MOVIE1_7T_AP              5  testretest1  817.5833   \n",
       "5                6  MOVIE2_7T_PA              1    inception   20.0000   \n",
       "6                7  MOVIE2_7T_PA              2    socialnet  266.7500   \n",
       "7                8  MOVIE2_7T_PA              3       oceans  545.3750   \n",
       "8                9  MOVIE2_7T_PA              4  testretest2  814.5833   \n",
       "9               10  MOVIE3_7T_PA              1       flower   20.0000   \n",
       "10              11  MOVIE3_7T_PA              2        hotel  220.5833   \n",
       "11              12  MOVIE3_7T_PA              3       garden  425.1250   \n",
       "12              13  MOVIE3_7T_PA              4       dreary  649.2500   \n",
       "13              14  MOVIE3_7T_PA              5  testretest3  811.5833   \n",
       "14              15  MOVIE4_7T_AP              1    homealone   20.0000   \n",
       "15              16  MOVIE4_7T_AP              2    brokovich  272.3333   \n",
       "16              17  MOVIE4_7T_AP              3     starwars  522.2083   \n",
       "17              18  MOVIE4_7T_AP              4  testretest4  797.5833   \n",
       "\n",
       "     stop_tr  \n",
       "0   264.0417  \n",
       "1   505.7083  \n",
       "2   713.7500  \n",
       "3   797.5417  \n",
       "4   900.9583  \n",
       "5   246.7083  \n",
       "6   525.3333  \n",
       "7   794.5833  \n",
       "8   897.9583  \n",
       "9   200.5417  \n",
       "10  405.0833  \n",
       "11  629.2083  \n",
       "12  791.7500  \n",
       "13  894.9583  \n",
       "14  252.2917  \n",
       "15  502.1667  \n",
       "16  777.3750  \n",
       "17  880.9583  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videoclip_table = pd.read_csv(\"data/videoclip_tr_lookup.csv\")\n",
    "videoclip_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In our dataset, each data sample is represented as a graph**  \n",
    "- each ROI is a node, i.e. we have 300 nodes in the graph\n",
    "- attribute of each node is its entire time-series\n",
    "- all nodes are connected pair wise\n",
    "- there is **no** edge attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading run 1/3\n",
      "loading run 2/3\n",
      "loading run 3/3\n"
     ]
    }
   ],
   "source": [
    "# Load dataset \n",
    "X, X_len, clip_y, num_subjs, num_clips = get_data()\n",
    "\n",
    "# Pad each time-series with zeros to equalize lengths\n",
    "X = pad_data(X)\n",
    "\n",
    "# Create graphs from data\n",
    "graphs_tuple = _clip_graphs(X)\n",
    "\n",
    "# Convert clip labels to one-hot vectors\n",
    "clip_y_oh = tf.one_hot(clip_y, num_clips).numpy()\n",
    "\n",
    "# Train, validation, test split\n",
    "num_train = 90 * num_clips\n",
    "num_val = 10 * num_clips\n",
    "num_test = 76 * num_clips\n",
    "\n",
    "train_G, train_y, val_G, val_y, test_G, test_y = train_val_test_split(graphs_tuple,\n",
    "                                                                       clip_y_oh,\n",
    "                                                                       num_train,\n",
    "                                                                       num_val,\n",
    "                                                                       num_test,\n",
    "                                                                       num_clips,\n",
    "                                                                       num_subjs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "<img src=\"NLNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Non-local neural network (**NLNN**) is a variant of Graph Network.\n",
    "- In our model, the **NLNN** takes as *input a graph* with node and edge attributes and *outputs a graph* with updated node attributes.\n",
    "\n",
    "**NLNN algorithm**\n",
    "1. For each edge $k$ in the input graph,  \n",
    "\n",
    "    - compute updated edge attributes  \n",
    "    $\\textbf{e}_{k}^{\\prime} = \\phi^{e} \\left( \\left[ \\textbf{v}_{s_k}, \\textbf{v}_{r_k} \\right] \\right)$,  \n",
    "    where $\\textbf{v}_{s_k}, \\textbf{v}_{r_k}$ are sender and receiver nodes attributes resp. of edge $k$,  \n",
    "    $\\left[ \\cdot, \\cdot \\right]$ denotes concatenation of attributes.  \n",
    "    \n",
    "2. For each vertex $i$ in the input graph,  \n",
    "\n",
    "    - aggregate attributes of all edges adjacent to node $i$  \n",
    "    $\\bar{\\textbf{e}}_{i}^{\\prime} = \\rho^{e \\rightarrow v} \\left( \\textbf{E}_{i}^{\\prime} \\right)$,  \n",
    "    where $\\textbf{E}_{i}^{\\prime}$ is set of received and sent edge attributes.  \n",
    "    \n",
    "    - compute updated node attributes    \n",
    "    $\\textbf{v}_{i}^{\\prime} = \\phi^{v} \\left( \\left[ \\textbf{v}_{i}, \\bar{\\textbf{e}}_{i}^{\\prime} \\right] \\right)$.  \n",
    "    \n",
    "**Update functions**\n",
    "1. $\\phi^{e}$: Single Layer Perceptron with output attribute size 16.  \n",
    "2. $\\phi^{v}$: Single Layer Perceptron with output attribute size 16.  \n",
    "\n",
    "**Aggregate functions**\n",
    "1. $\\rho^{e \\rightarrow v}$: summation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = NLNNClassifer(num_nodes=num_rois, \n",
    "                      k_linear=16, \n",
    "                      num_classes=num_clips)\n",
    "\n",
    "# Define loss and gradient functions\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def loss(model, x, y):\n",
    "  # training=training is needed only if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  y_ = model(x)\n",
    "  return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: Loss: 25.879, Accuracy: 86.759%, Validation set accuracy: 85.833%\n",
      "Epoch 001: Loss: 4.510, Accuracy: 98.241%, Validation set accuracy: 86.667%\n",
      "Epoch 002: Loss: 2.030, Accuracy: 99.815%, Validation set accuracy: 86.944%\n",
      "Epoch 003: Loss: 1.897, Accuracy: 99.444%, Validation set accuracy: 88.333%\n",
      "Epoch 004: Loss: 1.154, Accuracy: 99.815%, Validation set accuracy: 89.333%\n",
      "Epoch 005: Loss: 0.987, Accuracy: 99.907%, Validation set accuracy: 89.722%\n",
      "Epoch 006: Loss: 1.080, Accuracy: 100.000%, Validation set accuracy: 90.357%\n",
      "Epoch 007: Loss: 1.589, Accuracy: 99.630%, Validation set accuracy: 90.312%\n",
      "Epoch 008: Loss: 4.003, Accuracy: 99.722%, Validation set accuracy: 89.815%\n",
      "Epoch 009: Loss: 2.974, Accuracy: 100.000%, Validation set accuracy: 90.167%\n"
     ]
    }
   ],
   "source": [
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "val_loss_results = []\n",
    "val_accuracy_results = []\n",
    "\n",
    "template = \"Epoch {:03d}: \"+\n",
    "            \"Train Loss: {:.3f}, \"+\n",
    "            \"Train Accuracy: {:.3%}, \"+\n",
    "            \"Validation Loss: {:.3f}, \"+\n",
    "            \"Validation Accuracy: {:.3%}\"\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_avg = tf.keras.metrics.Mean()\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    val_loss_avg = tf.keras.metrics.Mean()\n",
    "    val_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "    # Training loop - using batches of <batch_size>\n",
    "    for i in range(0, num_train, batch_size):\n",
    "        \n",
    "        # Optimize the model\n",
    "        x = utils_tf.get_graph(train_G, slice(i, i + batch_size))\n",
    "        y = train_y[i : i + batch_size, :]\n",
    "        loss_value, grads = grad(model, x, y)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Track progress: Add current batch loss\n",
    "        train_loss_avg.update_state(loss_value)  \n",
    "        # Compare predicted label to actual label\n",
    "        train_accuracy.update_state(y, model(x))\n",
    "\n",
    "    # Log training loss and accuracy\n",
    "    train_loss_results.append(train_loss_avg.result())\n",
    "    train_accuracy_results.append(train_accuracy.result())\n",
    "    \n",
    "    # Validation loop\n",
    "    for i in range(0, num_val, batch_size):\n",
    "        x = utils_tf.get_graph(val_G, slice(i, i + batch_size))\n",
    "        y = val_y[i : i + batch_size, :]\n",
    "        logits = model(x)\n",
    "        \n",
    "        # Track progress\n",
    "        val_loss_value = loss(model, x, y)\n",
    "        val_loss_avg.update_state(val_loss_value)\n",
    "        val_accuracy.update_state(logits, y)\n",
    "        \n",
    "    # Log validation loss and accuracy\n",
    "    val_loss_results.append(val_loss_avg.result())\n",
    "    val_accuracy_results.append(val_accuracy.result())\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(template.format(epoch,\n",
    "                              epoch_loss_avg.result(),\n",
    "                              epoch_accuracy.result(),\n",
    "                              val_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "fig_tr, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig_tr.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(train_accuracy_results)\n",
    "plt.show()\n",
    "\n",
    "# Validation metrics\n",
    "fig_val, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig_val.suptitle('Validation Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(val_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(val_accuracy_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 92.377%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "for i in range(0, num_test, batch_size):\n",
    "    x = utils_tf.get_graph(test_G, slice(i, i + batch_size))\n",
    "    y = test_y[i : i + batch_size, :]\n",
    "    logits = model(x)\n",
    "    \n",
    "    test_accuracy.update_state(logits, y)\n",
    "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
