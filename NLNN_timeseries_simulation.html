

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Timeseries Simulation &#8212; Graph Networks on fMRI</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Breaking Down Classifier" href="NLNN_CC_Randomization.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  
  <h1 class="site-logo" id="site-title">Graph Networks on fMRI</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="NLNN_intro.html">
   What is NLNN?
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Experiments with NLNN
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="NLNN_Clip_Classifier.html">
   Clip Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NLNN_CC_Randomization.html">
   Breaking Down Classifier
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Timeseries Simulation
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/NLNN_timeseries_simulation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/codejoydo/graph_net"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/codejoydo/graph_net/issues/new?title=Issue%20on%20page%20%2FNLNN_timeseries_simulation.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/codejoydo/graph_net/edit/master/NLNN_timeseries_simulation.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-data">
   Load data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#create-graphs-from-data">
   Create graphs from data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-evaluation">
   Performance evaluation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-training">
     Model training
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="timeseries-simulation">
<h1>Timeseries Simulation<a class="headerlink" href="#timeseries-simulation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="load-data">
<h2>Load data<a class="headerlink" href="#load-data" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load dataset </span>
<span class="n">X</span><span class="p">,</span> <span class="n">X_len</span><span class="p">,</span> <span class="n">clip_y</span><span class="p">,</span> <span class="n">num_subjs</span><span class="p">,</span> <span class="n">num_clips</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">()</span>

<span class="c1"># clip names</span>
<span class="n">clip_name_to_idx</span> <span class="o">=</span> <span class="n">_get_clip_labels</span><span class="p">()</span>
<span class="n">clip_idx_to_name</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">clip_name_to_idx</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">clip_idx_to_name</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">clip_idx_to_name</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># get rid of run number in test-retest</span>
<span class="n">clip_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">clip_idx_to_name</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

<span class="c1"># # Pad each time-series with zeros to equalize lengths</span>
<span class="c1"># X = pad_data(X)</span>

<span class="c1"># Fix a clip for simulating its trajectory</span>
<span class="n">clip_num</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">X_clip</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">clip_y</span><span class="p">))</span> <span class="k">if</span> <span class="n">clip_y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">clip_num</span><span class="p">]</span>
<span class="n">y_clip</span> <span class="o">=</span> <span class="p">[</span><span class="n">clip_y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">clip_y</span><span class="p">))</span> <span class="k">if</span> <span class="n">clip_y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">clip_num</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loading run 1/1
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="create-graphs-from-data">
<h2>Create graphs from data<a class="headerlink" href="#create-graphs-from-data" title="Permalink to this headline">¶</a></h2>
<p>Here we use a history of length k = 5.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create data </span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># Length of memory/history of data-sequence </span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">idx_subj</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_subjs</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">X_clip</span><span class="p">[</span><span class="n">idx_subj</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">idx_tp</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">x_tp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx_tp</span> <span class="o">-</span> <span class="n">k</span> <span class="p">:</span> <span class="n">idx_tp</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">y_tp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx_tp</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_tp</span><span class="p">)</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_tp</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>        
        
<span class="c1"># Create graphs from data</span>
<span class="n">prob_edge</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">graphs_dict_list</span> <span class="o">=</span> <span class="n">clip_graphs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> 
                               <span class="n">prob_edge</span><span class="o">=</span><span class="n">prob_edge</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="performance-evaluation">
<h2>Performance evaluation<a class="headerlink" href="#performance-evaluation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="model-training">
<h3>Model training<a class="headerlink" href="#model-training" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train, test split</span>
<span class="n">num_X</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">num_train</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">num_X</span><span class="p">)</span>
<span class="n">num_val</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">*</span> <span class="n">num_X</span>
<span class="n">num_test</span> <span class="o">=</span> <span class="n">num_X</span> <span class="o">-</span> <span class="n">num_train</span> <span class="o">-</span> <span class="n">num_val</span>

<span class="n">num_splits</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">test_accuracy_results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx_split</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_splits</span><span class="p">):</span>
    
    <span class="c1"># Shuffle data for random train-test splits</span>
<span class="c1">#     rand_idx = np.rand_perm = np.random.permutation(num_X)</span>
<span class="c1">#     graphs_dict_list_perm = list(map(lambda i: graphs_dict_list[i], rand_idx))</span>
<span class="c1">#     y_perm = y[rand_idx, :]</span>
    <span class="n">graphs_dict_list_perm</span> <span class="o">=</span> <span class="n">graphs_dict_list</span>
    <span class="n">y_perm</span> <span class="o">=</span> <span class="n">y</span>
    
    <span class="c1"># Create split</span>
    <span class="n">train_G</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_G</span><span class="p">,</span> <span class="n">val_y</span><span class="p">,</span> <span class="n">test_G</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">train_val_test_split</span><span class="p">(</span><span class="n">graphs_dict_list_perm</span><span class="p">,</span>
                                                                           <span class="n">y_perm</span><span class="p">,</span>
                                                                           <span class="n">num_train</span><span class="p">,</span>
                                                                           <span class="n">num_val</span><span class="p">,</span>
                                                                           <span class="n">num_test</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">NLNNProcessDecode</span><span class="p">(</span><span class="n">num_nodes</span><span class="o">=</span><span class="n">train_G</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;nodes&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> 
                              <span class="n">k_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">],</span> 
                              <span class="n">num_processing_steps</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="n">graph_slice</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="p">:</span> <span class="n">utils_tf</span><span class="o">.</span><span class="n">data_dicts_to_graphs_tuple</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="n">end</span><span class="p">])</span>
    
    <span class="n">NLNN_simulator</span> <span class="o">=</span> <span class="n">base_model</span><span class="p">(</span><span class="n">task_type</span><span class="o">=</span><span class="s2">&quot;regression&quot;</span><span class="p">,</span>
                                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                                <span class="n">loss_object</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">MeanSquaredError</span><span class="p">(),</span>
                                <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">),</span>
                                <span class="n">eval_metric</span><span class="o">=</span><span class="n">tfa</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">RSquare</span><span class="p">(),</span>
                                <span class="n">eval_metric_name</span><span class="o">=</span><span class="s2">&quot;% var explained&quot;</span><span class="p">,</span>
                                <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                                <span class="n">slice_input</span><span class="o">=</span><span class="n">graph_slice</span><span class="p">)</span>
    
    <span class="n">results</span> <span class="o">=</span> <span class="n">NLNN_simulator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="o">=</span><span class="n">train_G</span><span class="p">,</span>
                                 <span class="n">train_Y</span><span class="o">=</span><span class="n">train_y</span><span class="p">,</span>
                                 <span class="n">val_X</span><span class="o">=</span><span class="n">test_G</span><span class="p">,</span>
                                 <span class="n">val_Y</span><span class="o">=</span><span class="n">test_y</span><span class="p">,</span>
                                 <span class="n">num_epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 000: Train Loss: 50.031, Train % var explained: -4868.926%  Val Loss: 14.078, Val % var explained: -1367.813%  
Epoch 001: Train Loss: 9.470, Train % var explained: -862.824%  Val Loss: 6.295, Val % var explained: -555.771%  
Epoch 002: Train Loss: 5.363, Train % var explained: -448.912%  Val Loss: 4.204, Val % var explained: -338.431%  
Epoch 003: Train Loss: 3.782, Train % var explained: -287.745%  Val Loss: 3.127, Val % var explained: -226.431%  
Epoch 004: Train Loss: 2.892, Train % var explained: -196.805%  Val Loss: 2.471, Val % var explained: -158.015%  
Epoch 005: Train Loss: 2.328, Train % var explained: -139.067%  Val Loss: 2.037, Val % var explained: -112.695%  
Epoch 006: Train Loss: 1.946, Train % var explained: -99.879%  Val Loss: 1.733, Val % var explained: -81.003%  
Epoch 007: Train Loss: 1.674, Train % var explained: -71.997%  Val Loss: 1.513, Val % var explained: -57.935%  
Epoch 008: Train Loss: 1.473, Train % var explained: -51.430%  Val Loss: 1.347, Val % var explained: -40.594%  
Epoch 009: Train Loss: 1.320, Train % var explained: -35.789%  Val Loss: 1.218, Val % var explained: -27.181%  
Epoch 010: Train Loss: 1.201, Train % var explained: -23.561%  Val Loss: 1.116, Val % var explained: -16.530%  
Epoch 011: Train Loss: 1.106, Train % var explained: -13.757%  Val Loss: 1.033, Val % var explained: -7.867%  
Epoch 012: Train Loss: 1.028, Train % var explained: -5.716%  Val Loss: 0.964, Val % var explained: -0.676%  
Epoch 013: Train Loss: 0.962, Train % var explained: 1.006%  Val Loss: 0.906, Val % var explained: 5.398%  
Epoch 014: Train Loss: 0.907, Train % var explained: 6.715%  Val Loss: 0.856, Val % var explained: 10.599%  
Epoch 015: Train Loss: 0.859, Train % var explained: 11.626%  Val Loss: 0.813, Val % var explained: 15.102%  
Epoch 016: Train Loss: 0.817, Train % var explained: 15.894%  Val Loss: 0.775, Val % var explained: 19.036%  
Epoch 017: Train Loss: 0.781, Train % var explained: 19.635%  Val Loss: 0.742, Val % var explained: 22.499%  
Epoch 018: Train Loss: 0.749, Train % var explained: 22.939%  Val Loss: 0.712, Val % var explained: 25.567%  
Epoch 019: Train Loss: 0.720, Train % var explained: 25.876%  Val Loss: 0.686, Val % var explained: 28.302%  
Epoch 020: Train Loss: 0.695, Train % var explained: 28.501%  Val Loss: 0.662, Val % var explained: 30.754%  
Epoch 021: Train Loss: 0.672, Train % var explained: 30.861%  Val Loss: 0.641, Val % var explained: 32.962%  
Epoch 022: Train Loss: 0.651, Train % var explained: 32.993%  Val Loss: 0.622, Val % var explained: 34.961%  
Epoch 023: Train Loss: 0.632, Train % var explained: 34.928%  Val Loss: 0.605, Val % var explained: 36.779%  
Epoch 024: Train Loss: 0.615, Train % var explained: 36.691%  Val Loss: 0.589, Val % var explained: 38.439%  
Epoch 025: Train Loss: 0.599, Train % var explained: 38.305%  Val Loss: 0.574, Val % var explained: 39.961%  
Epoch 026: Train Loss: 0.585, Train % var explained: 39.786%  Val Loss: 0.561, Val % var explained: 41.361%  
Epoch 027: Train Loss: 0.572, Train % var explained: 41.150%  Val Loss: 0.548, Val % var explained: 42.652%  
Epoch 028: Train Loss: 0.560, Train % var explained: 42.411%  Val Loss: 0.537, Val % var explained: 43.847%  
Epoch 029: Train Loss: 0.548, Train % var explained: 43.578%  Val Loss: 0.526, Val % var explained: 44.954%  
Epoch 030: Train Loss: 0.538, Train % var explained: 44.660%  Val Loss: 0.516, Val % var explained: 45.981%  
Epoch 031: Train Loss: 0.528, Train % var explained: 45.666%  Val Loss: 0.507, Val % var explained: 46.936%  
Epoch 032: Train Loss: 0.519, Train % var explained: 46.603%  Val Loss: 0.499, Val % var explained: 47.825%  
Epoch 033: Train Loss: 0.510, Train % var explained: 47.476%  Val Loss: 0.491, Val % var explained: 48.653%  
Epoch 034: Train Loss: 0.502, Train % var explained: 48.291%  Val Loss: 0.483, Val % var explained: 49.425%  
Epoch 035: Train Loss: 0.495, Train % var explained: 49.052%  Val Loss: 0.476, Val % var explained: 50.144%  
Epoch 036: Train Loss: 0.488, Train % var explained: 49.764%  Val Loss: 0.470, Val % var explained: 50.814%  
Epoch 037: Train Loss: 0.482, Train % var explained: 50.429%  Val Loss: 0.464, Val % var explained: 51.437%  
Epoch 038: Train Loss: 0.476, Train % var explained: 51.051%  Val Loss: 0.458, Val % var explained: 52.016%  
Epoch 039: Train Loss: 0.470, Train % var explained: 51.632%  Val Loss: 0.453, Val % var explained: 52.554%  
Epoch 040: Train Loss: 0.465, Train % var explained: 52.176%  Val Loss: 0.448, Val % var explained: 53.054%  
Epoch 041: Train Loss: 0.460, Train % var explained: 52.684%  Val Loss: 0.444, Val % var explained: 53.519%  
Epoch 042: Train Loss: 0.455, Train % var explained: 53.159%  Val Loss: 0.440, Val % var explained: 53.950%  
Epoch 043: Train Loss: 0.451, Train % var explained: 53.602%  Val Loss: 0.436, Val % var explained: 54.352%  
Epoch 044: Train Loss: 0.447, Train % var explained: 54.015%  Val Loss: 0.432, Val % var explained: 54.725%  
Epoch 045: Train Loss: 0.443, Train % var explained: 54.400%  Val Loss: 0.429, Val % var explained: 55.072%  
Epoch 046: Train Loss: 0.440, Train % var explained: 54.758%  Val Loss: 0.426, Val % var explained: 55.392%  
Epoch 047: Train Loss: 0.437, Train % var explained: 55.091%  Val Loss: 0.423, Val % var explained: 55.689%  
Epoch 048: Train Loss: 0.434, Train % var explained: 55.400%  Val Loss: 0.420, Val % var explained: 55.963%  
Epoch 049: Train Loss: 0.431, Train % var explained: 55.687%  Val Loss: 0.418, Val % var explained: 56.215%  
Epoch 050: Train Loss: 0.428, Train % var explained: 55.953%  Val Loss: 0.416, Val % var explained: 56.448%  
Epoch 051: Train Loss: 0.426, Train % var explained: 56.199%  Val Loss: 0.414, Val % var explained: 56.664%  
Epoch 052: Train Loss: 0.424, Train % var explained: 56.428%  Val Loss: 0.412, Val % var explained: 56.865%  
Epoch 053: Train Loss: 0.422, Train % var explained: 56.639%  Val Loss: 0.410, Val % var explained: 57.053%  
Epoch 054: Train Loss: 0.420, Train % var explained: 56.836%  Val Loss: 0.408, Val % var explained: 57.230%  
Epoch 055: Train Loss: 0.418, Train % var explained: 57.017%  Val Loss: 0.407, Val % var explained: 57.397%  
Epoch 056: Train Loss: 0.416, Train % var explained: 57.186%  Val Loss: 0.405, Val % var explained: 57.555%  
Epoch 057: Train Loss: 0.415, Train % var explained: 57.342%  Val Loss: 0.404, Val % var explained: 57.704%  
Epoch 058: Train Loss: 0.413, Train % var explained: 57.487%  Val Loss: 0.402, Val % var explained: 57.843%  
Epoch 059: Train Loss: 0.412, Train % var explained: 57.621%  Val Loss: 0.401, Val % var explained: 57.971%  
Epoch 060: Train Loss: 0.411, Train % var explained: 57.745%  Val Loss: 0.400, Val % var explained: 58.089%  
Epoch 061: Train Loss: 0.410, Train % var explained: 57.859%  Val Loss: 0.399, Val % var explained: 58.197%  
Epoch 062: Train Loss: 0.409, Train % var explained: 57.966%  Val Loss: 0.398, Val % var explained: 58.294%  
Epoch 063: Train Loss: 0.408, Train % var explained: 58.064%  Val Loss: 0.397, Val % var explained: 58.381%  
Epoch 064: Train Loss: 0.407, Train % var explained: 58.155%  Val Loss: 0.396, Val % var explained: 58.458%  
Epoch 065: Train Loss: 0.406, Train % var explained: 58.240%  Val Loss: 0.396, Val % var explained: 58.526%  
Epoch 066: Train Loss: 0.405, Train % var explained: 58.319%  Val Loss: 0.395, Val % var explained: 58.586%  
Epoch 067: Train Loss: 0.405, Train % var explained: 58.392%  Val Loss: 0.395, Val % var explained: 58.637%  
Epoch 068: Train Loss: 0.404, Train % var explained: 58.460%  Val Loss: 0.394, Val % var explained: 58.682%  
Epoch 069: Train Loss: 0.404, Train % var explained: 58.524%  Val Loss: 0.394, Val % var explained: 58.721%  
Epoch 070: Train Loss: 0.403, Train % var explained: 58.584%  Val Loss: 0.394, Val % var explained: 58.755%  
Epoch 071: Train Loss: 0.402, Train % var explained: 58.640%  Val Loss: 0.393, Val % var explained: 58.784%  
Epoch 072: Train Loss: 0.402, Train % var explained: 58.693%  Val Loss: 0.393, Val % var explained: 58.811%  
Epoch 073: Train Loss: 0.402, Train % var explained: 58.743%  Val Loss: 0.393, Val % var explained: 58.836%  
Epoch 074: Train Loss: 0.401, Train % var explained: 58.791%  Val Loss: 0.393, Val % var explained: 58.859%  
Epoch 075: Train Loss: 0.401, Train % var explained: 58.836%  Val Loss: 0.392, Val % var explained: 58.881%  
Epoch 076: Train Loss: 0.400, Train % var explained: 58.880%  Val Loss: 0.392, Val % var explained: 58.903%  
Epoch 077: Train Loss: 0.400, Train % var explained: 58.922%  Val Loss: 0.392, Val % var explained: 58.924%  
Epoch 078: Train Loss: 0.399, Train % var explained: 58.962%  Val Loss: 0.392, Val % var explained: 58.946%  
Epoch 079: Train Loss: 0.399, Train % var explained: 59.001%  Val Loss: 0.392, Val % var explained: 58.969%  
Epoch 080: Train Loss: 0.399, Train % var explained: 59.038%  Val Loss: 0.391, Val % var explained: 58.993%  
Epoch 081: Train Loss: 0.398, Train % var explained: 59.074%  Val Loss: 0.391, Val % var explained: 59.017%  
Epoch 082: Train Loss: 0.398, Train % var explained: 59.108%  Val Loss: 0.391, Val % var explained: 59.042%  
Epoch 083: Train Loss: 0.398, Train % var explained: 59.142%  Val Loss: 0.391, Val % var explained: 59.068%  
Epoch 084: Train Loss: 0.397, Train % var explained: 59.174%  Val Loss: 0.390, Val % var explained: 59.094%  
Epoch 085: Train Loss: 0.397, Train % var explained: 59.206%  Val Loss: 0.390, Val % var explained: 59.122%  
Epoch 086: Train Loss: 0.397, Train % var explained: 59.236%  Val Loss: 0.390, Val % var explained: 59.150%  
Epoch 087: Train Loss: 0.397, Train % var explained: 59.266%  Val Loss: 0.390, Val % var explained: 59.179%  
Epoch 088: Train Loss: 0.396, Train % var explained: 59.294%  Val Loss: 0.389, Val % var explained: 59.209%  
Epoch 089: Train Loss: 0.396, Train % var explained: 59.322%  Val Loss: 0.389, Val % var explained: 59.240%  
Epoch 090: Train Loss: 0.396, Train % var explained: 59.349%  Val Loss: 0.389, Val % var explained: 59.271%  
Epoch 091: Train Loss: 0.396, Train % var explained: 59.375%  Val Loss: 0.388, Val % var explained: 59.302%  
Epoch 092: Train Loss: 0.395, Train % var explained: 59.400%  Val Loss: 0.388, Val % var explained: 59.334%  
Epoch 093: Train Loss: 0.395, Train % var explained: 59.424%  Val Loss: 0.388, Val % var explained: 59.365%  
Epoch 094: Train Loss: 0.395, Train % var explained: 59.448%  Val Loss: 0.387, Val % var explained: 59.397%  
Epoch 095: Train Loss: 0.395, Train % var explained: 59.470%  Val Loss: 0.387, Val % var explained: 59.429%  
Epoch 096: Train Loss: 0.394, Train % var explained: 59.493%  Val Loss: 0.387, Val % var explained: 59.460%  
Epoch 097: Train Loss: 0.394, Train % var explained: 59.514%  Val Loss: 0.387, Val % var explained: 59.491%  
Epoch 098: Train Loss: 0.394, Train % var explained: 59.535%  Val Loss: 0.386, Val % var explained: 59.521%  
Epoch 099: Train Loss: 0.394, Train % var explained: 59.555%  Val Loss: 0.386, Val % var explained: 59.551%  
Epoch 100: Train Loss: 0.394, Train % var explained: 59.575%  Val Loss: 0.386, Val % var explained: 59.580%  
Epoch 101: Train Loss: 0.394, Train % var explained: 59.594%  Val Loss: 0.385, Val % var explained: 59.609%  
Epoch 102: Train Loss: 0.393, Train % var explained: 59.612%  Val Loss: 0.385, Val % var explained: 59.636%  
Epoch 103: Train Loss: 0.393, Train % var explained: 59.630%  Val Loss: 0.385, Val % var explained: 59.663%  
Epoch 104: Train Loss: 0.393, Train % var explained: 59.647%  Val Loss: 0.385, Val % var explained: 59.689%  
Epoch 105: Train Loss: 0.393, Train % var explained: 59.664%  Val Loss: 0.384, Val % var explained: 59.714%  
Epoch 106: Train Loss: 0.393, Train % var explained: 59.681%  Val Loss: 0.384, Val % var explained: 59.738%  
Epoch 107: Train Loss: 0.393, Train % var explained: 59.697%  Val Loss: 0.384, Val % var explained: 59.762%  
Epoch 108: Train Loss: 0.392, Train % var explained: 59.713%  Val Loss: 0.384, Val % var explained: 59.784%  
Epoch 109: Train Loss: 0.392, Train % var explained: 59.728%  Val Loss: 0.383, Val % var explained: 59.806%  
Epoch 110: Train Loss: 0.392, Train % var explained: 59.743%  Val Loss: 0.383, Val % var explained: 59.826%  
Epoch 111: Train Loss: 0.392, Train % var explained: 59.757%  Val Loss: 0.383, Val % var explained: 59.846%  
Epoch 112: Train Loss: 0.392, Train % var explained: 59.771%  Val Loss: 0.383, Val % var explained: 59.865%  
Epoch 113: Train Loss: 0.392, Train % var explained: 59.785%  Val Loss: 0.383, Val % var explained: 59.884%  
Epoch 114: Train Loss: 0.392, Train % var explained: 59.798%  Val Loss: 0.383, Val % var explained: 59.902%  
Epoch 115: Train Loss: 0.392, Train % var explained: 59.811%  Val Loss: 0.382, Val % var explained: 59.919%  
Epoch 116: Train Loss: 0.391, Train % var explained: 59.824%  Val Loss: 0.382, Val % var explained: 59.935%  
Epoch 117: Train Loss: 0.391, Train % var explained: 59.836%  Val Loss: 0.382, Val % var explained: 59.951%  
Epoch 118: Train Loss: 0.391, Train % var explained: 59.848%  Val Loss: 0.382, Val % var explained: 59.966%  
Epoch 119: Train Loss: 0.391, Train % var explained: 59.860%  Val Loss: 0.382, Val % var explained: 59.981%  
Epoch 120: Train Loss: 0.391, Train % var explained: 59.871%  Val Loss: 0.382, Val % var explained: 59.995%  
Epoch 121: Train Loss: 0.391, Train % var explained: 59.882%  Val Loss: 0.382, Val % var explained: 60.008%  
Epoch 122: Train Loss: 0.391, Train % var explained: 59.893%  Val Loss: 0.381, Val % var explained: 60.021%  
Epoch 123: Train Loss: 0.391, Train % var explained: 59.903%  Val Loss: 0.381, Val % var explained: 60.034%  
Epoch 124: Train Loss: 0.391, Train % var explained: 59.913%  Val Loss: 0.381, Val % var explained: 60.046%  
Epoch 125: Train Loss: 0.390, Train % var explained: 59.923%  Val Loss: 0.381, Val % var explained: 60.058%  
Epoch 126: Train Loss: 0.390, Train % var explained: 59.933%  Val Loss: 0.381, Val % var explained: 60.069%  
Epoch 127: Train Loss: 0.390, Train % var explained: 59.942%  Val Loss: 0.381, Val % var explained: 60.080%  
Epoch 128: Train Loss: 0.390, Train % var explained: 59.951%  Val Loss: 0.381, Val % var explained: 60.090%  
Epoch 129: Train Loss: 0.390, Train % var explained: 59.960%  Val Loss: 0.381, Val % var explained: 60.100%  
Epoch 130: Train Loss: 0.390, Train % var explained: 59.969%  Val Loss: 0.381, Val % var explained: 60.109%  
Epoch 131: Train Loss: 0.390, Train % var explained: 59.977%  Val Loss: 0.380, Val % var explained: 60.119%  
Epoch 132: Train Loss: 0.390, Train % var explained: 59.986%  Val Loss: 0.380, Val % var explained: 60.127%  
Epoch 133: Train Loss: 0.390, Train % var explained: 59.994%  Val Loss: 0.380, Val % var explained: 60.136%  
Epoch 134: Train Loss: 0.390, Train % var explained: 60.001%  Val Loss: 0.380, Val % var explained: 60.144%  
Epoch 135: Train Loss: 0.390, Train % var explained: 60.009%  Val Loss: 0.380, Val % var explained: 60.152%  
Epoch 136: Train Loss: 0.390, Train % var explained: 60.016%  Val Loss: 0.380, Val % var explained: 60.159%  
Epoch 137: Train Loss: 0.390, Train % var explained: 60.023%  Val Loss: 0.380, Val % var explained: 60.166%  
Epoch 138: Train Loss: 0.389, Train % var explained: 60.030%  Val Loss: 0.380, Val % var explained: 60.173%  
Epoch 139: Train Loss: 0.389, Train % var explained: 60.037%  Val Loss: 0.380, Val % var explained: 60.180%  
Epoch 140: Train Loss: 0.389, Train % var explained: 60.044%  Val Loss: 0.380, Val % var explained: 60.186%  
Epoch 141: Train Loss: 0.389, Train % var explained: 60.050%  Val Loss: 0.380, Val % var explained: 60.192%  
Epoch 142: Train Loss: 0.389, Train % var explained: 60.056%  Val Loss: 0.380, Val % var explained: 60.198%  
Epoch 143: Train Loss: 0.389, Train % var explained: 60.062%  Val Loss: 0.380, Val % var explained: 60.203%  
Epoch 144: Train Loss: 0.389, Train % var explained: 60.068%  Val Loss: 0.380, Val % var explained: 60.208%  
Epoch 145: Train Loss: 0.389, Train % var explained: 60.074%  Val Loss: 0.380, Val % var explained: 60.213%  
Epoch 146: Train Loss: 0.389, Train % var explained: 60.080%  Val Loss: 0.380, Val % var explained: 60.218%  
Epoch 147: Train Loss: 0.389, Train % var explained: 60.085%  Val Loss: 0.379, Val % var explained: 60.223%  
Epoch 148: Train Loss: 0.389, Train % var explained: 60.091%  Val Loss: 0.379, Val % var explained: 60.227%  
Epoch 149: Train Loss: 0.389, Train % var explained: 60.096%  Val Loss: 0.379, Val % var explained: 60.231%  
Epoch 150: Train Loss: 0.389, Train % var explained: 60.101%  Val Loss: 0.379, Val % var explained: 60.235%  
Epoch 151: Train Loss: 0.389, Train % var explained: 60.106%  Val Loss: 0.379, Val % var explained: 60.239%  
Epoch 152: Train Loss: 0.389, Train % var explained: 60.110%  Val Loss: 0.379, Val % var explained: 60.243%  
Epoch 153: Train Loss: 0.389, Train % var explained: 60.115%  Val Loss: 0.379, Val % var explained: 60.246%  
Epoch 154: Train Loss: 0.389, Train % var explained: 60.120%  Val Loss: 0.379, Val % var explained: 60.249%  
Epoch 155: Train Loss: 0.389, Train % var explained: 60.124%  Val Loss: 0.379, Val % var explained: 60.252%  
Epoch 156: Train Loss: 0.389, Train % var explained: 60.128%  Val Loss: 0.379, Val % var explained: 60.255%  
Epoch 157: Train Loss: 0.388, Train % var explained: 60.132%  Val Loss: 0.379, Val % var explained: 60.258%  
Epoch 158: Train Loss: 0.388, Train % var explained: 60.136%  Val Loss: 0.379, Val % var explained: 60.261%  
Epoch 159: Train Loss: 0.388, Train % var explained: 60.140%  Val Loss: 0.379, Val % var explained: 60.264%  
Epoch 160: Train Loss: 0.388, Train % var explained: 60.144%  Val Loss: 0.379, Val % var explained: 60.266%  
Epoch 161: Train Loss: 0.388, Train % var explained: 60.148%  Val Loss: 0.379, Val % var explained: 60.268%  
Epoch 162: Train Loss: 0.388, Train % var explained: 60.152%  Val Loss: 0.379, Val % var explained: 60.271%  
Epoch 163: Train Loss: 0.388, Train % var explained: 60.155%  Val Loss: 0.379, Val % var explained: 60.273%  
Epoch 164: Train Loss: 0.388, Train % var explained: 60.159%  Val Loss: 0.379, Val % var explained: 60.275%  
Epoch 165: Train Loss: 0.388, Train % var explained: 60.162%  Val Loss: 0.379, Val % var explained: 60.277%  
Epoch 166: Train Loss: 0.388, Train % var explained: 60.165%  Val Loss: 0.379, Val % var explained: 60.279%  
Epoch 167: Train Loss: 0.388, Train % var explained: 60.169%  Val Loss: 0.379, Val % var explained: 60.280%  
Epoch 168: Train Loss: 0.388, Train % var explained: 60.172%  Val Loss: 0.379, Val % var explained: 60.282%  
Epoch 169: Train Loss: 0.388, Train % var explained: 60.175%  Val Loss: 0.379, Val % var explained: 60.284%  
Epoch 170: Train Loss: 0.388, Train % var explained: 60.178%  Val Loss: 0.379, Val % var explained: 60.285%  
Epoch 171: Train Loss: 0.388, Train % var explained: 60.181%  Val Loss: 0.379, Val % var explained: 60.287%  
Epoch 172: Train Loss: 0.388, Train % var explained: 60.183%  Val Loss: 0.379, Val % var explained: 60.288%  
Epoch 173: Train Loss: 0.388, Train % var explained: 60.186%  Val Loss: 0.379, Val % var explained: 60.289%  
Epoch 174: Train Loss: 0.388, Train % var explained: 60.189%  Val Loss: 0.379, Val % var explained: 60.290%  
Epoch 175: Train Loss: 0.388, Train % var explained: 60.191%  Val Loss: 0.379, Val % var explained: 60.292%  
Epoch 176: Train Loss: 0.388, Train % var explained: 60.194%  Val Loss: 0.379, Val % var explained: 60.293%  
Epoch 177: Train Loss: 0.388, Train % var explained: 60.196%  Val Loss: 0.379, Val % var explained: 60.294%  
Epoch 178: Train Loss: 0.388, Train % var explained: 60.199%  Val Loss: 0.379, Val % var explained: 60.295%  
Epoch 179: Train Loss: 0.388, Train % var explained: 60.201%  Val Loss: 0.379, Val % var explained: 60.296%  
Epoch 180: Train Loss: 0.388, Train % var explained: 60.203%  Val Loss: 0.379, Val % var explained: 60.296%  
Epoch 181: Train Loss: 0.388, Train % var explained: 60.206%  Val Loss: 0.379, Val % var explained: 60.297%  
Epoch 182: Train Loss: 0.388, Train % var explained: 60.208%  Val Loss: 0.379, Val % var explained: 60.298%  
Epoch 183: Train Loss: 0.388, Train % var explained: 60.210%  Val Loss: 0.379, Val % var explained: 60.299%  
Epoch 184: Train Loss: 0.388, Train % var explained: 60.212%  Val Loss: 0.379, Val % var explained: 60.300%  
Epoch 185: Train Loss: 0.388, Train % var explained: 60.214%  Val Loss: 0.379, Val % var explained: 60.300%  
Epoch 186: Train Loss: 0.388, Train % var explained: 60.216%  Val Loss: 0.379, Val % var explained: 60.301%  
Epoch 187: Train Loss: 0.388, Train % var explained: 60.218%  Val Loss: 0.379, Val % var explained: 60.301%  
Epoch 188: Train Loss: 0.388, Train % var explained: 60.220%  Val Loss: 0.379, Val % var explained: 60.302%  
Epoch 189: Train Loss: 0.388, Train % var explained: 60.222%  Val Loss: 0.379, Val % var explained: 60.303%  
Epoch 190: Train Loss: 0.388, Train % var explained: 60.223%  Val Loss: 0.379, Val % var explained: 60.303%  
Epoch 191: Train Loss: 0.388, Train % var explained: 60.225%  Val Loss: 0.379, Val % var explained: 60.304%  
Epoch 192: Train Loss: 0.388, Train % var explained: 60.227%  Val Loss: 0.379, Val % var explained: 60.304%  
Epoch 193: Train Loss: 0.387, Train % var explained: 60.229%  Val Loss: 0.379, Val % var explained: 60.304%  
Epoch 194: Train Loss: 0.387, Train % var explained: 60.230%  Val Loss: 0.379, Val % var explained: 60.305%  
Epoch 195: Train Loss: 0.387, Train % var explained: 60.232%  Val Loss: 0.379, Val % var explained: 60.305%  
Epoch 196: Train Loss: 0.387, Train % var explained: 60.233%  Val Loss: 0.379, Val % var explained: 60.306%  
Epoch 197: Train Loss: 0.387, Train % var explained: 60.235%  Val Loss: 0.379, Val % var explained: 60.306%  
Epoch 198: Train Loss: 0.387, Train % var explained: 60.236%  Val Loss: 0.379, Val % var explained: 60.306%  
Epoch 199: Train Loss: 0.387, Train % var explained: 60.238%  Val Loss: 0.379, Val % var explained: 60.307%  
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_loss_results</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">train_pve_results</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">test_loss_results</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">test_pve_results</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

<span class="c1"># Training metrics</span>
<span class="n">fig_tr</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">fig_tr</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Training Metrics&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;log(Loss)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">train_loss_results</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Percentage Variance Explained&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_pve_results</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Validation metrics</span>
<span class="n">fig_val</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">fig_val</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Validation Metrics&#39;</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;log(Loss)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">test_loss_results</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Percentage Variance Explained&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_pve_results</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/NLNN_timeseries_simulation_10_0.png" src="_images/NLNN_timeseries_simulation_10_0.png" />
<img alt="_images/NLNN_timeseries_simulation_10_1.png" src="_images/NLNN_timeseries_simulation_10_1.png" />
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="NLNN_CC_Randomization.html" title="previous page">Breaking Down Classifier</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Govinda and Joyneel<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="_static/js/index.js"></script>
    
  </body>
</html>